{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n",
    "\n",
    "This notebook reviews a paper from 2016 by the [New England Microsoft Research Team and Boston University](https://arxiv.org/pdf/1607.06520.pdf) titled _Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings_. From the title alone, you can gather that the problem they were trying to solve was to try and remove unnecessary gender typing from word embeddings, that are unfortunate side-effects to unsupervised learning. In this notebook, you'll be able to play around with some of the popular word embedding implementations, and see how the problem outlined in the paper can manifest itself.\n",
    "\n",
    "---\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "To start with, we'll give a brief overview of what word embeddings are. If we want to automate some kind of analysis, we have to represent whatever data we are working with into a format that machines can operate on. Whether that be language translation, drug discovery, or self-driving cars, we have to be able to compress the data into some compact vector format that captures all of the relevant information needed to perform the analysis/make decisions. \n",
    "\n",
    "As an example, translating from English to French by machine learning would need the word \"library\" to be translated into a vector of bits, processed by some kind of model (even linear regression!), which returns the analogous vector in \"computer French\", which could then be looked up in a dictionary (literally) to give the equivalent word in French.\n",
    "\n",
    "```\n",
    "# english to french translation\n",
    "\"library\" -> [0, 1, 0...] -> model -> [1, 0, 1...] -> \"bibliothèque\"\n",
    "```\n",
    "\n",
    "The process of going from word to vector is referred to as _encoding_; the vector itself is also called an encoding and sometimes embedding interchangably. Word embeddings are a method of turning words—as strings—into set-length vectors of features that can subsequently be used for machine analysis. The idea behind these representations is to allow comparisons of words in a continuous space, for example:\n",
    "\n",
    "$$ \\mathrm{King - Queen = Man - Woman} $$\n",
    "\n",
    "with the right kind of embedding, you would be able to perform logical operations on text data that ultimately gives us applications like machine translation models sentiment classification. The tricky thing is to learn useful embeddings that can span extremely large corpuses (the dictionary of words), and maintain the same degree of usefulness even with words outside the training corpus. The advantage of machine learning is not needing to hand-code all of these heuristics into your encoding scheme. The idea is to let some algorithm develop its _own heuristic_, either in a supervised or unsupervised way. In the former, you will have specific examples to train the model against; in the above example, maybe a regression model that learns to encode words based on their distances from other words (i.e. \"elephant\" vs. \"dog\" vs. \"car\"). In the latter, the model develops its own heuristics (hence unsupervised) based on whatever is necessary to perform some task. The unsupervised case is attractive both because you don't need specific labeled examples, and also because the language model will devise its own heuristics: there will be edge cases I as a human will have missed that the model has a chance of covering - providing there are enough examples of it).\n",
    "\n",
    "## Unsupervised encoding\n",
    "\n",
    "So how would you train a model in an unsupervised way, that manages to capture all the nuances about language, such as context, meaning, syntax, and grammar? The most straightforward way is to just give a model sentences to reproduce: given a sentence, encode each word into vectors, and use the same vectors to recreate the sentence accurately. In this way, a model will self-penalize when the wrong word is used in the wrong place, and by updating how the vectors are generated, you eventually end up with a model that maps words into vectors that recall the right word in the right place (hopefully most of the time).\n",
    "\n",
    "One of the first really successful attempts at this was by [Bengio _et al._ in 2003](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) showed that a small neural network could actually learn embeddings in an unsupervised fashion:\n",
    "\n",
    "![nnlm](bengio_nnlm.png)\n",
    "\n",
    "The neural network learns an embedding (the earlier layers) by predicting the correct word, given a context. \n",
    "\n",
    "The problem with the earlier approach is the computational inefficiency of the output softmax layer ($\\exp(w_t)/\\sum \\exp(w_t)$) and its set dimensionality: the embedding diversity only grows with the context and word diversity, and if it's set at 10,000 words (or any number for that matter) this sets a roof to the level of complexity we can reach.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "A more modern approach, and now one of the most widely used methods of generating embeddings is the `word2vec` technique developed by Mikolov _et al._ [(paper)](https://arxiv.org/abs/1301.3781).  In summary, a small neural network is taught to transform words into a 300-dimensional embedding based on a 1.6 billion word dataset; a year or so later they trained on a 100 billion word dataset. The architecture that performed best of two was the continuous-bag-of-words (CBOW), which was trained to predict the most likely word, given a context (e.g. Jane visits _blank_ in September). Over training, the network learns to produce embeddings that encode how words are used in various contexts, represented within the 300-dimensional vector. If you're wondering why the choice of a 300-dimensional embedding, the authors tested larger embeddings, and did not see huge improvements—if you think in terms of basis vectors, convergence of this basis becomes logarithmic near this size for this architecture. \n",
    "\n",
    "![cbow](cbow-skipgram.png)\n",
    "\n",
    "You can play around with `word2vec` below, by loading pre-trained models/corpuses. For more information about these datasets, see [this link](https://github.com/RaRe-Technologies/gensim-data). A very lightweight model is \"text8\", which is a couple of megabytes. One of the largest, and the one under scrutiny is the model trained on the Google News' feed, usually referred to as `w2vnews`. The interesting effect of training on various sources is the semanticity that these models learn to produce in embeddings: writing style is \"absorbed\" into the model, and so the context/quality of embeddings from \"text8\" is significantly different from `w2vnews`; the former is more \"academic\" while the latter reflects writing in modern society more."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!pip install gensim"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gensim in /home/kelvin/anaconda3/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: boto3 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.12.37)\n",
      "Requirement already satisfied: requests in /home/kelvin/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-storage in /home/kelvin/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.27.0)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.37 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.37)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.13.1)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.3.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.37->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.37->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (46.0.0.post20200309)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.51.0)\n",
      "Requirement already satisfied: pytz in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (2019.3)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/kelvin/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.11.3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# This can get loud, so only run if you're interested in trying out word2vec!\n",
    "RUN = True\n",
    "\n",
    "if RUN:\n",
    "    # Text8 is the first 100,000,000 bytes in Wikipedia\n",
    "    corpus = api.load(\"text8\")\n",
    "    model = Word2Vec(corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"text8\" is trained on 17 million words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model.corpus_total_words"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can retrieve embeddings by using the `model.wv.get_vector` method:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "king = model.wv.get_vector(\"king\")\n",
    "man = model.wv.get_vector(\"man\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Word2Vec interface gives access to a bunch of convenience functions too, like looking up the corpus based on cosine similarity:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "model.wv.similar_by_vector(king - man)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('king', 0.7000098824501038),\n",
       " ('dukes', 0.5468707084655762),\n",
       " ('vii', 0.5334069132804871),\n",
       " ('kings', 0.5269961357116699),\n",
       " ('sultan', 0.5252265930175781),\n",
       " ('umayyad', 0.5200234055519104),\n",
       " ('iii', 0.5178303718566895),\n",
       " ('aragon', 0.5035035610198975),\n",
       " ('antiochus', 0.49747395515441895),\n",
       " ('princes', 0.49325495958328247)]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "model.wv.cosine_similarities(king - man, [model.wv.get_vector(\"queen\")])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.4165773], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on this example, it's obvious that getting useful embeddings is a difficult task: the embeddings depend strongly on context, and based on 17 million words in Wikipedia, this is still not enough get embeddings for such a high level concept."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can either train your own `word2vec` model on your own corpus, or use a pre-trained model (e.g. [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing), it's 1.5GB though!). Applications-wise you could use transfer learning from one of these large pre-trained models for your own application (for example academic translation), or distillation learning to get a model working."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## The problem with word embeddings\n",
    "\n",
    "Since these language models are based on written text, one issue that became apparent to the authors of the [NIPS paper](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf) is an unintended side-effect where these embeddings learn gender biases. The problem itself extends outside of gender, where there is just an inherent, underlying bias towards particular contexts that are not intentional. This is particularly a problem for gender when these embeddings are used for real world applications that can have profound effect on peoples' lives, such as resume scanners and review/report generation. This direct quote from the paper:\n",
    "\n",
    "> However, none of these papers have recognized how blatantly sexist the embeddings are and hence risk introducing biases of various types into real-world systems.\n",
    "\n",
    "This depends somewhat strongly on the embedding you use, although the authors contend that it's not just Google's large news model.\n",
    "\n",
    "Based on the idea behind `word2vec`, the unsupervised learning forces the model to produce embeddings based on contexts it has seen. While the focus of the paper is on the effect this has on embeddings, it probably is a better reflection on how society writes news. \n",
    "\n",
    "To demonstrate, we're first going to try and extract the \"gender\" aspect of the embedding. One way that comes to mind is simply to take opposite gender words, and take the difference of these embeddings; for example, $\\mathrm{man} - \\mathrm{woman}$—the concept of gender should be the dominant residual context (if the embedding has been generated well)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# This step actually downloads the w2vnews model; \n",
    "# if you don't want to download 1.6 GB don't run this\n",
    "RUN = False\n",
    "\n",
    "if RUN:\n",
    "    w2vnews = api.load(\"word2vec-google-news-300\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "g = w2vnews.get_vector(\"woman\") - w2vnews.get_vector(\"man\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The residual vector, $g$, should have the concept of gender encoded in it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "g.size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test this out, you can loop over a set of common names that have gender encoded:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "similarities = list()\n",
    "names = [\"john\", \"jesse\", \"mary\", \"jessie\", \"elton\", \"kelvin\", \"katie\", \"charles\"]\n",
    "for name in names:\n",
    "    name_vec = w2vnews.get_vector(name)\n",
    "    similarity = w2vnews.cosine_similarities(name_vec, [g])\n",
    "    similarities.append(similarity)\n",
    "    print(name, w2vnews.cosine_similarities(name_vec, [g]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "john [-0.03143504]\n",
      "jesse [-0.03190297]\n",
      "mary [0.09950283]\n",
      "jessie [0.01639201]\n",
      "elton [0.01321754]\n",
      "kelvin [-0.07067004]\n",
      "katie [0.12954304]\n",
      "charles [-0.06644631]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "Y = np.arange(len(similarities))\n",
    "\n",
    "ax.scatter(similarities, Y)\n",
    "for x, y, name in zip(similarities, Y, names):\n",
    "    ax.text(x, y + 0.2, name, horizontalalignment=\"center\")\n",
    "ax.axvline(0., ls=\"--\", alpha=0.6)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7f8e20ab2690>"
      ]
     },
     "metadata": {},
     "execution_count": 89
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What you see here is predominantly male names are negative, while female names are positive (except for Elton?). This operation is basically showing you which side of an axis (the \"gender\" axis) you are on with a given word. With names and other gender-appropriate/specific words this is fine, but when you start applying it to other word embeddings it becomes a problem:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "professions = [\"homemaker\", \"programmer\", \"doctor\", \"nurse\", \"receptionist\", \"captain\", \"boss\"]\n",
    "for profession in professions:\n",
    "    name_vec = w2vnews.get_vector(profession)\n",
    "    print(profession,  w2vnews.cosine_similarities(name_vec, [g]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "homemaker [0.33099553]\n",
      "programmer [-0.03050756]\n",
      "doctor [0.16264015]\n",
      "nurse [0.33768806]\n",
      "receptionist [0.32569164]\n",
      "captain [-0.13851051]\n",
      "boss [-0.13585725]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interestingly, the cosine similarities for these words are much more strongly gendered than names. The problem you can see manifests itself in a few words like boss, captain that are strongly biased towards male, whereas nurse and receptionist are female."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution: project out the gender embedding\n",
    "\n",
    "The key here to simply calculate a vector of $g$ that reliably encodes the gender dimension of our problem, and simply use this vector to project out any words that do not have a gender appropriate component; for example, names like John and Mary are appropriately gender-specific, and so is grandma and brother. Conversely, words like model, shopping, and football should nominally have no gender component, although there are some gray areas like \"widwife\"...\n",
    "\n",
    "The bulk of the paper works out various methods of removing gender bias that are latent in these word embeddings. One way was to utilize vector math to project out the gender dimension from words that should not have significant bias, and the research was into defining a vector, $g$, that embodies the gender component of a 300-dimensional vector best."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![basisfigure](fig2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Projection operator\n",
    "\n",
    "Obtain $\\vec{v_\\mathrm{gender}}$ as the gender component of $\\vec{v}$, the word vector of interest, by projecting it onto the \"gender\" axis (I use quotations here because it's probably not a purely gender dimension, but a mixture of different features closely associated with gender).\n",
    "\n",
    "$$ \\vec{v_\\mathrm{gender}} = \\frac{\\vec{v} \\cdot \\vec{g}}{\\vert \\vert \\vec{g} \\vert \\vert^2_2} $$ \n",
    "\n",
    "The \"debiased\" vector, $\\vec{v_\\mathrm{debias}}$, is given by subtracting out the gender part of the embedding,\n",
    "\n",
    "$$\\vec{v_\\mathrm{debias}} = \\vec{v} - \\vec{v_\\mathrm{gender}} $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "target_word = \"philosopher\"\n",
    "target_vector = w2vnews.get_vector(target_word)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# Similarity before we project out gender\n",
    "unprojected_similarity = w2vnews.cosine_similarities(target_vector, [g])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def project_ab(a: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    Project vector a onto vector b; takes two 1D NumPy arrays as input.\n",
    "    \"\"\"\n",
    "    return np.dot(a, b) / np.linalg.norm(b, ord=2)**2.\n",
    "\n",
    "\n",
    "def debias_word(wordvec: np.ndarray, gender_embedding: np.ndarray):\n",
    "    \"\"\"\n",
    "    Subtract out the gender dimension from the embedding. Uses a reference\n",
    "    gender embedding vector to perform the projection.\n",
    "    \"\"\"\n",
    "    return wordvec - project_ab(wordvec, gender_embedding)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "debiased = debias_word(target_vector, g)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "debiased_professions = [debias_word(w2vnews.get_vector(profession), g) for profession in professions]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "for profession, vec in zip(professions, debiased_professions):\n",
    "    print(profession, w2vnews.cosine_similarities(vec, [g]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "homemaker [0.08230869]\n",
      "programmer [-0.02580311]\n",
      "doctor [0.0722914]\n",
      "nurse [0.08140509]\n",
      "receptionist [0.08034401]\n",
      "captain [-0.06835929]\n",
      "boss [-0.06978016]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare with the values above, and you'll see that while the projection is not complete (it won't go to zero due to mixing in the basis vector), we have largely elminated the $g$ contamination in our word embedding. As to whether or not this adversely affects performance of these word embeddings, the authors showed that this projection does not change the performance of word analogies and other uses significantly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusions\n",
    "\n",
    "Due to the nature of unsupervised learning of word embeddings, you have these unfortunate side-effects that we should be cognizant about: outside of gender, you can imagine that there are similar biases towards other concepts that may affect production models. Beyond word embeddings, you can also imagine this can happen to any sort of autoencoder model, which learns to reproduce data but not necessarily always the way you want it to; we should be cognizant of this when applying these kinds of models to problems, and it pays to understand how your encoding/embedding is formed by visualizing and testing it.\n",
    "\n",
    "\n",
    "### Further reading\n",
    "\n",
    "[GLoVE model](https://nlp.stanford.edu/pubs/glove.pdf), a more recent model for word embeddings.\n",
    "\n",
    "[Grad-CAM](http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), a visualization technique for convolutional neural networks.\n",
    "\n",
    "[Men also like shopping: Reducing gender bias amplification using corpus-level constraints](https://arxiv.org/abs/1707.09457), a more recent paper on removing gender bias from word embeddings.\n",
    "\n",
    "[Using adverserial learning to mitigate stereotyping](https://dl.acm.org/doi/abs/10.1145/3278721.3278779)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 understanding-ml",
   "language": "python",
   "name": "uml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}